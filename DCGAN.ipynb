{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN Implementation\n",
    "\n",
    "Implementation of the (vanilla) Deep Convolutional Generative Adversarial Network defined by <a href=\"https://arxiv.org/pdf/1511.06434.pdf\">Radford, Metz, and Chintala (2016)</a>.  Testing is done on the <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10</a> benchmark image dataset, stored in pickled format in the `data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to convert CIFAR-10 to Pytorch Dataset\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as in_file:\n",
    "        pickle_dict = pickle.load(in_file, encoding='bytes')\n",
    "    return pickle_dict\n",
    "\n",
    "\n",
    "def read_data(file_name):\n",
    "    \"\"\"Given path to CIFAR data batch, returns raw X, y tensors.\"\"\"\n",
    "    \n",
    "    batch_dict = unpickle(file_name)\n",
    "    X_raw = torch.tensor(batch_dict[b'data'])\n",
    "    y_raw = torch.tensor(batch_dict[b'labels']).long()\n",
    "    return X_raw, y_raw\n",
    "\n",
    "\n",
    "def shape_image(X):\n",
    "    \"\"\"Reshapes raw data tensor to nn.module-compatible RGB image\"\"\"\n",
    "    \n",
    "    # Each row of X_raw contains RGB color channels concatenated in row-major order\n",
    "    # Need to first split channels into dim 1 on tensor, then shape dim 2/3 into image\n",
    "    image_size = 32*32\n",
    "    X = torch.split(X.unsqueeze(dim=1), image_size, dim=2)\n",
    "    X = torch.cat(X, dim=1)\n",
    "    X = X.view(-1, 3, 32, 32)   # (N, channels, pixel rows, pixel cols)\n",
    "    return X\n",
    "\n",
    "\n",
    "def normalize(X, a=-1, b=1):\n",
    "    \"\"\"Normalizes data tensor to [a, b] using min-max scaling.\"\"\"\n",
    "    \n",
    "    data_min = torch.min(X).float().item()\n",
    "    data_max = torch.max(X).float().item()\n",
    "    assert a < b, \"Rescaled range [a, b] must have a < b\"\n",
    "    \n",
    "    # First scale to [0, 1], then rescale to [a, b]\n",
    "    X = (X - data_min) / (data_max - data_min)\n",
    "    X = (X * (b - a)) + a  \n",
    "    return X\n",
    "\n",
    "\n",
    "class CIFARDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom Dataset class which preprocesses and stores datasets\n",
    "       from CIFAR batch files.  Works for CIFAR-10 and CIFAR-100.\"\"\"\n",
    "    \n",
    "    def __init__(self, X=None, y=None):\n",
    "        self.data = X\n",
    "        self.labels = y\n",
    "        \n",
    "    def load(self, file_list):\n",
    "        # Get list of (X, y) tuples, concatenate corresponding tensors\n",
    "        combined_list = [read_data(file_name) for file_name in file_list]\n",
    "        X_list, y_list = list(zip(*combined_list))\n",
    "        X = torch.cat(X_list, dim=0)\n",
    "        y = torch.cat(y_list, dim=0)\n",
    "        \n",
    "        self.data = normalize(shape_image(X))\n",
    "        self.labels = y\n",
    "        return self\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Generates an (X, y) pair at given index\n",
    "        return self.data[index], self.labels[index]\n",
    "    \n",
    "    def cuda(self):\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            self.data = self.data.to(device)\n",
    "            self.labels = self.labels.to(device)\n",
    "            \n",
    "    def cpu(self):\n",
    "        self.data = self.data.cpu()\n",
    "        self.labels = self.labels.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CIFARDataset().load([f\"data/data_batch_{n}\" for n in range(1, 6)])\n",
    "test_set = CIFARDataset().load([\"data/test_batch\"])\n",
    "cifar_meta = unpickle(\"data/batches.meta\")\n",
    "\n",
    "# Visualize a few random examples\n",
    "def plot_image(x, ax):\n",
    "    \"\"\"Helper to scale and plot output from ImageDataset.\"\"\"\n",
    "    image = (x.squeeze().permute(1, 2, 0) + 1) / 2\n",
    "    ax.imshow(image.clone().cpu().detach())\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "for i in range(3):\n",
    "    X, y = train_set[i*1000]\n",
    "    plot_image(X, ax[i])\n",
    "    ax[i].set_title(cifar_meta[b'label_names'][y.item()].decode('ascii'))\n",
    "plt.suptitle(\"CIFAR-10 sample images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define architecture for vanilla generator and discriminator\n",
    "# See also:\n",
    "# https://gluon.mxnet.io/chapter14_generative-adversarial-networks/dcgan.html\n",
    "# https://wandb.ai/sairam6087/dcgan/reports/DCGAN-on-CIFAR-10--Vmlldzo5NjMyOQ\n",
    "# https://github.com/soumith/ganhacks\n",
    "\n",
    "def conv_block(which_model, conv_args=[], conv_kwargs={'bias': False},\n",
    "               leak_slope=0.02, batch_norm=True, activation=True):\n",
    "    \"\"\"Performs all the operations in a single DCGAN layer in the following order:\n",
    "       batch normalization -> convolution (optional) -> nonlinearity (optional)\n",
    "       \n",
    "       If which == 'gen', does a transpose convolution\n",
    "       If which == 'dis', does a normal convolution\n",
    "       Returns a list of functions in order; should unpack and fill in nn.Sequential\"\"\"\n",
    "    \n",
    "    funcs = []\n",
    "    \n",
    "    if which_model == 'gen':\n",
    "        funcs.append(nn.ConvTranspose2d(*conv_args, **conv_kwargs))\n",
    "        if batch_norm:\n",
    "            funcs.append(nn.BatchNorm2d(conv_args[1], affine=False))\n",
    "        if activation:\n",
    "            funcs.append(nn.ReLU())\n",
    "            \n",
    "    elif which_model == 'dis':\n",
    "        funcs.append(nn.Conv2d(*conv_args, **conv_kwargs))\n",
    "        if batch_norm:\n",
    "            funcs.append(nn.BatchNorm2d(conv_args[1], affine=False))\n",
    "        if activation:\n",
    "            funcs.append(nn.LeakyReLU(leak_slope))\n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Argument `which_model` is not a valid value\")\n",
    "        \n",
    "    # Initialize the weights to match DCGAN paper?\n",
    "    # Build a list of the three functions in sequence\n",
    "    return funcs\n",
    "    \n",
    "\n",
    "class DCGenerator(nn.Module):\n",
    "    \"\"\"Deep convolutional generator which maps latent noise vector -> (32,32) RGB-channel image.\n",
    "       The latent space input z is projected and convolved with many feature maps.\n",
    "       Subsequent layers use only fractional-strided convolutions (no pooling).\n",
    "       All hidden layers use ReLU activation, and output layer uses tanh.\"\"\"\n",
    "    \n",
    "    def __init__(self, z_len=100):\n",
    "        \n",
    "        super(DCGenerator, self).__init__()\n",
    "        self.z_len = z_len\n",
    "        \n",
    "        self.in_layer = nn.Linear(z_len, 128 * 4 * 4)\n",
    "        \n",
    "        # Choose kernel_size, stride, padding to double height/width for each layer\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            #*conv_block('gen', [z_len, 128, 4, 1, 0]),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            *conv_block('gen', [128, 128, 4, 2, 1]),\n",
    "            *conv_block('gen', [128, 128, 4, 2, 1]),\n",
    "            *conv_block('gen', [128, 3, 4, 2, 1], batch_norm=False, activation=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # Reshape z as 4d tensor\n",
    "        z = self.in_layer(z).view(-1, 128, 4, 4)\n",
    "        return self.conv_layers(z)\n",
    "\n",
    "\n",
    "class DCDiscriminator(nn.Module):\n",
    "    \"\"\"Deep convolutional discriminator which maps (32,32) RGB-channel image -> [0, 1].\n",
    "       The image is passed through several convolutional layers (again, no pooling).\n",
    "       All hidden layers use LeakyReLU activation, and output layer uses sigmoid.\"\"\"\n",
    "    \n",
    "    def __init__(self, leak_slope=0.02):\n",
    "        \n",
    "        super(DCDiscriminator, self).__init__()\n",
    "        \n",
    "        # Just reverse the convolutional layers from the Generator\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            *conv_block('dis', [3, 128, 4, 2, 1], batch_norm=False),\n",
    "            *conv_block('dis', [128, 128, 4, 2, 1]),\n",
    "            *conv_block('dis', [128, 128, 4, 2, 1])\n",
    "        )\n",
    "        \n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(128 * 4 * 4, 100),\n",
    "            nn.LeakyReLU(leak_slope),\n",
    "            nn.Linear(100, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # Reshape discriminator output by flattening, then feed into sigmoid activation\n",
    "        x = x.view(-1, torch.prod(torch.tensor(x.shape[1:])))\n",
    "        return self.out_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineGenerator(nn.Module):\n",
    "    def __init__(self, nz=100, nc=3, ngf=64):\n",
    "        super(OnlineGenerator, self).__init__()\n",
    "        self.z_len = nz\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 32 x 32\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.main(z)\n",
    "    \n",
    "    \n",
    "class OnlineDiscriminator(nn.Module):\n",
    "    def __init__(self, nc=3, ndf=64):\n",
    "        super(OnlineDiscriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 32 x 32\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 16 x 16\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 8 x 8\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 4 x 4\n",
    "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = OnlineGenerator()\n",
    "dis = OnlineDiscriminator()\n",
    "z_test = torch.rand(5, 100, 1, 1)\n",
    "gen(z_test).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN Training\n",
    "\n",
    "The training is done using the minimax game described in the original GAN paper by <a href=\"https://arxiv.org/pdf/1406.2661.pdf\">Goodfellow, et al (2014)</a>.  The choice of optimizer and batch size are specific to the DCGAN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to train built models and evaluate convergence\n",
    "\n",
    "def jensen_shannon(which_model, device):\n",
    "    \"\"\"Returns a function to compute an empirical estimate of the Jensen-Shannon\n",
    "       divergence between the true data-generating distribution P_real and\n",
    "       the generated distribution P_model over parallel batches x_real and x_model.\n",
    "       \n",
    "       This metric is optimized by a minimax operation, first maximizing over the\n",
    "       discriminator weights, then minimizing over the generator weights.\n",
    "       \n",
    "       Returns:\n",
    "       - if which_model = 'dis': returns function (D(x_model), D(x_real)) -> scalar\n",
    "       - if which_model = 'gen': returns function (D(x_model)) -> scalar\n",
    "         Both functions are returned in the form of minimization problems.\n",
    "       - if which_model = 'full': returns non-negative version of 'dis' function.\n",
    "         Use this version to calculate divergence after an epoch.\"\"\"\n",
    "    \n",
    "    bce = nn.BCELoss()\n",
    "    \n",
    "    # Functions to compute respective discriminator labels\n",
    "    # Consider label smoothing: draw some value near 0 or 1\n",
    "    fake = lambda d: 0.1 * torch.ones(d.size()).to(device)\n",
    "    real = lambda d: 0.9 * torch.ones(d.size()).to(device)\n",
    "    \n",
    "    if which_model == 'dis':\n",
    "        #return lambda dis_model, dis_real: \\\n",
    "            #-torch.mean(torch.log(dis_real) + torch.log(1 - dis_model))  \n",
    "        model_loss = lambda d_model: bce(d_model, fake(d_model))\n",
    "        real_loss = lambda d_real: bce(d_real, real(d_real))\n",
    "        return lambda d_model, d_real: model_loss(d_model) + real_loss(d_real)\n",
    "        \n",
    "    elif which_model == 'gen':\n",
    "        # If generated samples are rejected easily, log(1 - D(x_model)) saturates\n",
    "        # (stays close to 0), so use an approximation to improve early training\n",
    "        #return lambda dis_model: \\\n",
    "            #-torch.mean(torch.log(dis_model))\n",
    "        return lambda d_model: bce(d_model, real(d_model))\n",
    "    \n",
    "    elif which_model == 'full':\n",
    "        return lambda dis_model, dis_real: \\\n",
    "            torch.mean(torch.log(dis_real) + torch.log(1 - dis_model))\n",
    "   \n",
    "    else:\n",
    "        raise ValueError(\"Argument `which_model` is not a valid value\")\n",
    "\n",
    "        \n",
    "def train_DCGAN(gen, dis, train_set, test_set,\n",
    "                num_epochs=100, dg_ratio=1, batch_size=128,\n",
    "                use_cuda=True, print_every=1):\n",
    "    \"\"\"Simultaneously trains generator and discriminator using minimax optimization of\n",
    "       Jensen-Shannon divergence between discriminator performance on x_real vs. x_model.\n",
    "       Uses ADAM optimizer for both networks, using params defined in DCGAN paper.\n",
    "       Latent codes Z are drawn from a normal prior.\n",
    "       \n",
    "       Parameters\n",
    "       - num_epochs: the number of training epochs over the dataset\n",
    "       - dg_ratio: the ratio of discriminator batches to generator batches\n",
    "       - batch_size: the train_set batch size passed into discriminator\n",
    "                     (for each real batch, an equal-sized batch is generated by generator)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move to GPU if possible; batches get moved as needed to save on memory\n",
    "    device = \"cuda:0\" if use_cuda and torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    criterion_dis = jensen_shannon(\"dis\", device)\n",
    "    criterion_gen = jensen_shannon(\"gen\", device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_dis = torch.optim.Adam(dis.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_gen = torch.optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "    # Keep a global count of how many discriminator steps happen per generator step\n",
    "    # because dg_ratio may not divide evenly into batch_size\n",
    "    dis_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        #print(f\"EPOCH {epoch+1}\")\n",
    "        \n",
    "        for X, y in loader:\n",
    "            z = torch.randn(X.shape[0], gen.z_len, 1, 1).to(device)\n",
    "            X = X.clone().to(device, non_blocking=True)\n",
    "            fake = torch.zeros(X.shape[0]).to(device)\n",
    "            real = torch.ones(X.shape[0]).to(device)\n",
    "            \n",
    "            # Discriminator trains each batch\n",
    "            optimizer_dis.zero_grad()\n",
    "            #loss_dis = criterion_dis(dis(gen(z)), dis(X))\n",
    "            #loss_dis.backward()\n",
    "            dis_real = dis(X).view(-1)\n",
    "            loss_dis_real = criterion(dis_real, real)\n",
    "            loss_dis_real.backward()\n",
    "            dis_fake = dis(gen(z).detach()).view(-1)\n",
    "            loss_dis_fake = criterion(dis_fake, fake)\n",
    "            loss_dis_fake.backward()\n",
    "            loss_dis = loss_dis_real + loss_dis_fake\n",
    "            optimizer_dis.step()\n",
    "            dis_count += 1\n",
    "            \n",
    "            # Generator trains if enough discriminator passes have gone through\n",
    "            if dis_count >= dg_ratio:\n",
    "                #z = torch.randn(batch_size, gen.z_len).to(device)\n",
    "                \n",
    "                optimizer_gen.zero_grad()\n",
    "                #loss_gen = criterion_gen(dis(gen(z)))\n",
    "                dis_fake = dis(gen(z)).view(-1)\n",
    "                loss_gen = criterion(dis_fake, real)\n",
    "                loss_gen.backward()\n",
    "                optimizer_gen.step()\n",
    "                dis_count = 0\n",
    " \n",
    "        # Evaluate how the model is performing on test set after a full epoch\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f\"- EPOCH {epoch+1}:\" +\n",
    "                  f\"\\n  discriminator loss = {loss_dis}\" +\n",
    "                  f\"\\n      generator loss = {loss_gen}\" +\n",
    "                  \"\\n----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = OnlineGenerator().cuda()\n",
    "dis = OnlineDiscriminator().cuda()\n",
    "train_set = CIFARDataset().load([f\"data/data_batch_{n}\" for n in range(1, 2)])\n",
    "test_set = CIFARDataset().load([\"data/test_batch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_DCGAN(gen, dis, train_set, test_set, dg_ratio=1, num_epochs=50, print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = torch.randn(1000, gen.z_len, 1, 1).cuda()\n",
    "gen_z_test = gen(z_test)\n",
    "best_idx = torch.randperm(1000)[:16]\n",
    "\n",
    "fig, ax = plt.subplots(4, 4, figsize=(8, 8))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        plot_image(gen(z_test)[best_idx[4*i + j]], ax[i][j])\n",
    "        #plot_image(train_set[1][0], ax[1])\n",
    "plt.tight_layout()\n",
    "\n",
    "#print(f\"Fake images discriminator:\\n{dis(gen(z_test[:10]))}\\n\")\n",
    "#print(f\"Real images discriminator:\\n{dis(train_set[:10][0].cuda())}\\n\")\n",
    "print(f\"Possible labels: {[label.decode('ascii') for label in cifar_meta[b'label_names']]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(gen, \"models/online_gen\")\n",
    "#torch.save(dis, \"models/online_dis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional DCGANs\n",
    "\n",
    "As an attempt to stabilize the model, we implement a training procedure for the conditional version of the DCGAN architecture above, based on the original <a href=\"https://arxiv.org/pdf/1411.1784.pdf\">Conditional GAN</a> paper and <a href=\"https://arxiv.org/pdf/1606.03498.pdf\">Salimans, et al (2016)</a>.  Whereas the vanilla GAN optimizes the objective function\n",
    "$$\n",
    "\\min_G \\max_D V(G, D)\n",
    "    = \\mathbb{E}_{\\mathbf{x} \\sim p_{real}(\\mathbf{x})} \\log D(\\mathbf{x})\n",
    "    + \\mathbb{E}_{\\mathbf{z} \\sim p_{model}(\\mathbf{x})} \\log (1-D(G(\\mathbf{z})))\n",
    "$$\n",
    "the conditional GAN simply adjusts this to\n",
    "$$\n",
    "\\min_G \\max_D V(G, D)\n",
    "    = \\mathbb{E}_{\\mathbf{x} \\sim p_{real}(\\mathbf{x})} \\log D(\\mathbf{x} | \\mathbf{y})\n",
    "    + \\mathbb{E}_{\\mathbf{z} \\sim p_{model}(\\mathbf{x})} \\log (1-D(G(\\mathbf{z} | \\mathbf{y})))\n",
    "$$\n",
    "where $\\mathbf{y}$ is some auxiliary vector (in this case, the CIFAR-10 labels).\n",
    "\n",
    "This can be accomplished by simply concatenating the auxiliary vector $\\mathbf{y}$ with a dense encoding in both the generator (at the input layer) and the discriminator (at the output layer).  The loss function Below is essentially a generalized version of the code above which allows for labels.  Setting `y_len = 0` and `num_classes = 1` recovers the unconditional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional generalizations of some of the architecture/training functions\n",
    "# http://cs231n.stanford.edu/reports/2015/pdfs/jgauthie_final_report.pdf\n",
    "\n",
    "class CDCGenerator(DCGenerator):\n",
    "    \"\"\"CONDITIONAL deep convolutional generator which maps\n",
    "       latent noise vector z, auxiliary y -> (32,32) RGB-channel image.\n",
    "       The latent space input z is projected and convolved with many feature maps.\n",
    "       Subsequent layers use only fractional-strided convolutions (no pooling).\"\"\"\n",
    "    \n",
    "    def __init__(self, z_len=100, y_len=10, code_len=5):\n",
    "        super().__init__(z_len)\n",
    "        self.y_len = y_len\n",
    "        self.y_encode = nn.Linear(y_len, code_len)\n",
    "        # Modify the parent architecture to accept larger input\n",
    "        self.in_layer = nn.Linear(z_len + code_len, self.in_layer.out_features)\n",
    "        \n",
    "    def forward(self, z, y):\n",
    "        # Encode y to avoid sparse representation\n",
    "        y_code = self.y_encode(y)\n",
    "        # Concatenate z and y, reshape as 4d tensor\n",
    "        z = torch.cat([z, y_code], dim=1)\n",
    "        z = self.in_layer(z).view(-1, 128, 4, 4)\n",
    "        return self.conv_layers(z)\n",
    "    \n",
    "\n",
    "class CDCDiscriminator(DCDiscriminator):\n",
    "    \"\"\"CONDITIONAL deep convolutional discriminator which maps\n",
    "       (32,32) RGB-channel image, auxiliary y -> [0, 1].\n",
    "       The image is passed through several convolutional layers (again, no pooling).\n",
    "       All hidden layers use LeakyReLU activation, and output layer uses sigmoid.\"\"\"\n",
    "    \n",
    "    def __init__(self, leak_slope=0.02, y_len=10, code_len=5):\n",
    "        super().__init__(leak_slope)\n",
    "        self.y_len = y_len\n",
    "        self.y_encode = nn.Linear(y_len, code_len)\n",
    "        # Modify the parent architecture to allow concatenating in final layer\n",
    "        self.out_layer[2] = nn.Linear(self.out_layer[2].in_features + code_len, 1)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # Encode y to avoid sparse representation\n",
    "        y_code = self.y_encode(y)\n",
    "        x = self.conv_layers(x)\n",
    "        # Reshape discriminator output by flattening\n",
    "        x = x.view(-1, torch.prod(torch.tensor(x.shape[1:])))\n",
    "        x = self.out_layer[:2](x)\n",
    "        # Concatenate y at the final stage, before sigmoid activation\n",
    "        x = torch.cat([x, y_code], dim=1)\n",
    "        return self.out_layer[2:](x)\n",
    "\n",
    "\n",
    "def onehot(y, num_classes, as_float=True):\n",
    "    \"\"\"Returns a one-hot encoding version of 1d vector y.\"\"\"\n",
    "    \n",
    "    y_onehot = torch.zeros(len(y), num_classes, dtype=torch.long)\n",
    "    y_onehot[range(len(y)), y] = 1\n",
    "    if as_float:\n",
    "        y_onehot = y_onehot.float()\n",
    "    return y_onehot\n",
    "\n",
    "    \n",
    "def cond_jensen_shannon(which_model, device):\n",
    "    \"\"\"Returns a function to compute an empirical estimate of the Jensen-Shannon\n",
    "       divergence between the true data-generating distribution P_real and\n",
    "       the generated distribution P_model over parallel batches x_real and x_model,\n",
    "       CONDITIONED on auxiliary vector y.\n",
    "       \n",
    "       This metric is optimized by a minimax operation, first maximizing over the\n",
    "       discriminator weights, then minimizing over the generator weights.\n",
    "       \n",
    "       Returns:\n",
    "       - if which_model = 'dis': returns function (D(x_model), D(x_real), y) -> scalar\n",
    "       - if which_model = 'gen': returns function (D(x_model), y) -> scalar\n",
    "         Both functions are returned in the form of minimization problems.\n",
    "       - if which_model = 'full': returns non-negative version of 'dis' function.\n",
    "         Use this version to calculate divergence after an epoch.\"\"\"\n",
    "    \n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Functions to compute respective discriminator labels\n",
    "    fake = lambda d: torch.zeros(d.size(), dtype=torch.long).to(device)\n",
    "    \n",
    "    if which_model == 'dis':\n",
    "        #return lambda dis_model, dis_real: \\\n",
    "            #-torch.mean(torch.log(dis_real) + torch.log(1 - dis_model))  \n",
    "        model_loss = lambda d_model, y: ce(d_model, 0 * y)\n",
    "        real_loss = lambda d_real, y: ce(d_real, y)\n",
    "        return lambda d_model, d_real, y: model_loss(d_model, y) + real_loss(d_real, y)\n",
    "        \n",
    "    elif which_model == 'gen':\n",
    "        # If generated samples are rejected easily, log(1 - D(x_model)) saturates\n",
    "        # (stays close to 0), so use an approximation to improve early training\n",
    "        #return lambda dis_model: \\\n",
    "            #-torch.mean(torch.log(dis_model))\n",
    "        return lambda d_model, y: ce(d_model, y)\n",
    "   \n",
    "    else:\n",
    "        raise ValueError(\"Argument `which_model` is not a valid value\")\n",
    "        \n",
    "\n",
    "def train_CDCGAN(gen, dis, train_set, test_set, num_classes=1,\n",
    "                num_epochs=100, dg_ratio=1, batch_size=128,\n",
    "                use_cuda=True, print_every=1):\n",
    "    \"\"\"Simultaneously trains generator and discriminator using minimax optimization of\n",
    "       Jensen-Shannon divergence between discriminator performance on x_real vs. x_model.\n",
    "       Uses ADAM optimizer for both networks, using params defined in DCGAN paper.\n",
    "       Latent codes Z are drawn from a uniform prior.\n",
    "       \n",
    "       Parameters\n",
    "       - num_epochs: the number of training epochs over the dataset\n",
    "       - dg_ratio: the ratio of discriminator batches to generator batches\n",
    "       - batch_size: the train_set batch size passed into discriminator\n",
    "                     (for each real batch, an equal-sized batch is generated by generator)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move to GPU if possible; batches get moved as needed to save on memory\n",
    "    device = \"cuda:0\" if use_cuda and torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    criterion_dis = jensen_shannon(\"dis\", device)\n",
    "    criterion_gen = jensen_shannon(\"gen\", device)\n",
    "    optimizer_dis = torch.optim.Adam(dis.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_gen = torch.optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "    # Keep a global count of how many discriminator steps happen per generator step\n",
    "    # because dg_ratio may not divide evenly into batch_size\n",
    "    dis_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        #print(f\"EPOCH {epoch+1}\")\n",
    "        \n",
    "        for X, y in loader:\n",
    "            z = torch.randn((X.shape[0], gen.z_len)).to(device)\n",
    "            # Sample random y values from distribution of labels\n",
    "            y_rand = np.random.choice(num_classes, size=X.shape[0])\n",
    "            y_rand_onehot = onehot(y_rand, num_classes).float().to(device)\n",
    "            \n",
    "            X = X.clone().to(device, non_blocking=True)\n",
    "            y = y.clone().to(device, non_blocking=True)\n",
    "            y_onehot = onehot(y, num_classes).float().to(device)\n",
    "            \n",
    "            # Discriminator trains each batch\n",
    "            optimizer_dis.zero_grad()\n",
    "            loss_dis = criterion_dis(dis(gen(z, y_rand_onehot), y_rand_onehot),\n",
    "                                     dis(X, y_onehot))\n",
    "            loss_dis.backward()\n",
    "            optimizer_dis.step()\n",
    "            dis_count += 1\n",
    "            \n",
    "            # Generator trains if enough discriminator passes have gone through\n",
    "            if dis_count >= dg_ratio:\n",
    "                z = torch.randn((X.shape[0], gen.z_len)).to(device)\n",
    "                \n",
    "                optimizer_gen.zero_grad()\n",
    "                loss_gen = criterion_gen(dis(gen(z, y_rand_onehot), y_rand_onehot))\n",
    "                loss_gen.backward()\n",
    "                optimizer_gen.step()\n",
    "                dis_count = 0\n",
    " \n",
    "        # Evaluate how the model is performing on test set after a full epoch\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f\"- EPOCH {epoch+1}:\" +\n",
    "                  f\"\\n  discriminator loss = {loss_dis}\" +\n",
    "                  f\"\\n      generator loss = {loss_gen}\" +\n",
    "                  \"\\n----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgen = CDCGenerator(y_len=len(label_set))\n",
    "cdis = CDCDiscriminator(y_len=len(label_set))\n",
    "\n",
    "z_test = torch.randn(15, cgen.z_len)\n",
    "y_rand = np.random.choice(len(label_set), size=15)\n",
    "X, y = train_set[:15]\n",
    "y_onehot = onehot(y, cgen.y_len)\n",
    "\n",
    "cdis(cgen(z_test, y_onehot), y_onehot).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CIFARDataset().load([f\"data/data_batch_{n}\" for n in range(1, 2)])\n",
    "test_set = CIFARDataset().load([\"data/test_batch\"])\n",
    "label_set = [label.decode('ascii') for label in cifar_meta[b'label_names']]\n",
    "\n",
    "cgen = CDCGenerator(y_len=len(label_set)).cuda()\n",
    "cdis = CDCDiscriminator(y_len=len(label_set)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_CDCGAN(cgen, cdis, train_set, test_set, num_classes=len(label_set), \\\n",
    "                   dg_ratio=1, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = torch.randn(1, cgen.z_len).cuda()\n",
    "X, y = train_set[:1]\n",
    "y_onehot = onehot(y, cgen.y_len).cuda()\n",
    "fake_X = cgen(z_test, y_onehot)\n",
    "fake_y = torch.randperm(cgen.y_len)[0]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "plot_image(fake_X[0], ax[0])\n",
    "plot_image(X[0], ax[1])\n",
    "ax[0].set_title(f\"{label_set[fake_y.item()]}?\")\n",
    "ax[1].set_title(label_set[y[0].item()])\n",
    "\n",
    "print(f\"Possible labels: {[label.decode('ascii') for label in cifar_meta[b'label_names']]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Generated Images\n",
    "\n",
    "The main metric we use to evaluate how closely the generated images reflect the training set is the *Frechet inception distance.*  This is a distance metric which compares statistics of samples drawn from the $P_{model}$ and $P_{real}$ distributions.  For corresponding sets of samples $\\{x^{(1)}_{model}, ..., x^{(N)}_{model}\\}$ and $\\{x^{(1)}_{real}, ..., x^{(N)}_{real}\\}$, each $x^{(i)}$ is projected into a 2048-dimensional feature space by running a forward pass through an <a href=\"https://arxiv.org/pdf/1512.00567.pdf\">Inception v3</a> convolutional net pre-trained on the ImageNet dataset, which here is acting essentially as an autoencoder.  This network maps tensors of size $(N, 3, 299, 299) \\to (N, 1, 1, 100)$.  So the CIFAR-10 images have to first be zero-padded to the $(299, 299)$ ImageNet size, and then we have to slightly modify the model to output the penultimate layer of size $(N, 1, 1, 2048)$.\n",
    "\n",
    "These embedded vectors are assumed to obey some multivariate Gaussian distribution, such that $x^{(i)}_{emb, model} \\overset{\\text{iid}}{\\sim} \\mathcal{N}_{2048}(\\mu_1, \\Sigma_1)$ and $x^{(i)}_{emb, real} \\overset{\\text{iid}}{\\sim} \\mathcal{N}_{2048}(\\mu_2, \\Sigma_2)$.  Then, the FID is the statistical Frechet distance between these two vectors:\n",
    "$$ FID = ||\\mu_1 - \\mu_2||^2_2 + tr[\\Sigma_1 + \\Sigma_2 - 2(\\Sigma_1 \\Sigma_2)^{1/2}] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3\n",
    "from scipy.linalg import sqrtm\n",
    "from torch.nn.modules.upsampling import Upsample\n",
    "from similaritymeasures import frechet_dist\n",
    "\n",
    "iv3 = inception_v3(pretrained=True, aux_logits=False)\n",
    "# Remove the final layer from the forward pass\n",
    "iv3.fc = nn.Identity()\n",
    "# Add a non-weighted upsampling layer to make this compatible to CIFAR-10\n",
    "upsample = Upsample(scale_factor=299/32)\n",
    "\n",
    "# FID runs considerably faster if this is loaded on GPU\n",
    "if torch.cuda.is_available():\n",
    "    iv3.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fid(fake_set, real_set, num_samples=1000, batch_size=128):\n",
    "    \"\"\"Calculates the Frechet Inception Distance (FID) between two empirical distributions.\n",
    "       Both `fake_set` and `real_set` should be CIFARDatasets which can be data loaded.\n",
    "       Computes distance over min{`num_samples`, min{len(fake_set), len(real_set)}}\n",
    "       random samples drawn from respective datasets.\n",
    "       \n",
    "       It really helps to run this on a GPU, if possible.\"\"\"\n",
    "    \n",
    "    min_len = min(len(fake_set), len(real_set))\n",
    "    num_samples = min(num_samples, min_len)\n",
    "    random_idx = torch.randperm(min_len)[:num_samples]\n",
    "    \n",
    "    # Pick some random layer of Inception v3 to check device\n",
    "    device = iv3.Conv2d_1a_3x3.conv.weight.device\n",
    "    \n",
    "    embeds = [torch.empty(num_samples, 2048), torch.empty(num_samples, 2048)]\n",
    "    \n",
    "    for dataset, i in zip([fake_set, real_set], [0, 1]):\n",
    "        loader = torch.utils.data.DataLoader(CIFARDataset(*dataset[random_idx]), batch_size)\n",
    "        \n",
    "        # Run the encoding in batches to avoid memory overload\n",
    "        for batch_num, (samples, _) in enumerate(loader):\n",
    "            # Necessary to stop gradients from accumulating in iv3 forward passes\n",
    "            with torch.no_grad():\n",
    "                samples = samples.to(device)\n",
    "                # (N, 3, 32, 32) -> (N, 2048)\n",
    "                embedded = iv3(upsample(samples))\n",
    "                samples = samples.cpu()\n",
    "                embeds[i][batch_num*batch_size:(batch_num+1)*batch_size, :] = embedded   \n",
    "    \n",
    "    # Use pre-compiled discrete Frechet distance function\n",
    "    return frechet_dist(embeds[0], embeds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
